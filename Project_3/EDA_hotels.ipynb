{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":34288,"databundleVersionId":3207826,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ===   EDA. Predict rating hotels  ===","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries and Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.feature_selection import chi2, f_classif\nimport category_encoders as ce\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Download necessary resources for sentiment analysis\nnltk.download('vader_lexicon')\n\n# Display settings\npd.set_option('display.max_columns', None)\nsns.set(style=\"whitegrid\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load and Combine Train and Test Data","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-booking/'\n\ndf_train = pd.read_csv(DATA_DIR + '/hotels_train.csv')\ndf_test = pd.read_csv(DATA_DIR + '/hotels_test.csv')\nsample_submission = pd.read_csv(DATA_DIR + '/submission.csv')\n\n# Add sample flags\ndf_train['sample'] = 1  # training data\ndf_test['sample'] = 0   # test data\ndf_test['reviewer_score'] = 0  # placeholder for target\n\n# Combine train and test for consistent preprocessing\ndata = pd.concat([df_test, df_train], axis=0).reset_index(drop=True)\nprint(f\"Combined dataset shape: {data.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initial Exploration & Cleaning","metadata":{}},{"cell_type":"code","source":"# Create a copy for feature engineering\nn_hotels = data.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#data['hotel_name'].nunique()\n# Проверим на наличие дубликатов и отфильтруем их \n\nL0=len(n_hotels)\nn_hotels.drop_duplicates(inplace=True)\nL1=len(n_hotels)\n\nprint('Обнаружено {} дубликатов'.format(L0-L1))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of unique hotels:\", n_hotels['hotel_name'].nunique())\n\n# Convert review date to datetime format\nn_hotels['review_date'] = pd.to_datetime(n_hotels['review_date'])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Строим тепловую карту, где желтым отмечены пропущеные значения\ncbar_kws = { 'ticks' : [0,1] }\ncolors = ['blue', 'yellow'] \nfig = plt.figure(figsize=(10, 4))\nax = sns.heatmap(\n    n_hotels.T.isnull(), # Создаем карту пропущенных значений в базе данных\n    cmap=sns.color_palette(colors),\n    xticklabels=False,\n    cbar_kws = cbar_kws\n)\nax.set_title('Пропущенные данные в базе');","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observation:** There missing lat lng for some positions in our data set","metadata":{}},{"cell_type":"markdown","source":"## Filling in the gaps in geographic coordinates. (Feature generation)","metadata":{}},{"cell_type":"code","source":"n_hotels.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels[n_hotels['lat'].isnull()][['hotel_name', 'hotel_address']]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels[['hotel_name', 'hotel_address', 'lat', 'lng']].head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сначала подключим необходимые библиотеку и модуль для поиска географических координат \n# #!pip install geopy\n\n#from geopy.geocoders import Nominatim\n#geolocator = Nominatim(user_agent=\"AzureMaps\")\n#location = geolocator.geocode( n_hotels.iloc[0]['hotel_address'])\n\n#  Проверка\n#print(location.address)\n#print((location.latitude, location.longitude))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n#  Step: Filling Missing Geographic Coordinates\n# =====================================================\n\n# Install geopy if not available\n# !pip install geopy\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\n\n# Initialize geocoder (use a descriptive user agent to avoid blocking)\ngeolocator = Nominatim(user_agent=\"HotelGeoFinder\")\n\n# Add a rate limiter to avoid \"Too Many Requests\" errors\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n\nmissing_coords = n_hotels[n_hotels['lng'].isnull()]['hotel_address'].unique()\nprint(f\"Found {len(missing_coords)} addresses with missing coordinates.\")\n\n# Define helper functions with error handling\ndef get_lat(address):\n    try:\n        shortaddress = \" \".join(address.split()[-3:])  # use last 3 elements (City, Region, Country)\n        location = geocode(shortaddress)\n        return location.latitude if location else np.nan\n    except Exception as e:\n        print(f\"Error getting latitude for {address}: {e}\")\n        return np.nan\n\ndef get_lng(address):\n    try:\n        shortaddress = \" \".join(address.split()[-3:])\n        location = geocode(shortaddress)\n        return location.longitude if location else np.nan\n    except Exception as e:\n        print(f\"Error getting longitude for {address}: {e}\")\n        return np.nan\n\n# Build dictionaries to map addresses → coordinates\nlat_dict = {addr: get_lat(addr) for addr in missing_coords}\nlng_dict = {addr: get_lng(addr) for addr in missing_coords}\n\n# Fill missing values in main dataset\nn_hotels['lat'] = n_hotels['lat'].fillna(n_hotels['hotel_address'].map(lat_dict))\nn_hotels['lng'] = n_hotels['lng'].fillna(n_hotels['hotel_address'].map(lng_dict))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nn_hotels[n_hotels['lat'].isnull()][['hotel_name', 'hotel_address']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_country(address):\n    \"\"\"Extract country name from a hotel address string.\"\"\"\n    if pd.isna(address) or address.strip() == '':\n        return None\n    \n    # Split by comma — country is usually the last segment\n    parts = address.split(',')\n    last_part = parts[-1].strip() if len(parts) > 0 else address.strip()\n\n    # Handle cases where address might not use commas\n    if ' ' in last_part:\n        # In some addresses, country may be the last word\n        return last_part.split()[-1].strip()\n    else:\n        return last_part\n\n# Apply the extraction\nn_hotels['hotel_country'] = n_hotels['hotel_address'].apply(extract_country)\n\n# Quick sanity check\nprint(n_hotels['hotel_country'].value_counts().head(20))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"country_fix = {\n    'Kingdom': 'United Kingdom',\n    'Spain': 'Spain',\n    'France': 'France',\n    'Netherlands': 'Netherlands',\n    'Austria': 'Austria',\n    'Italy': 'Italy'\n}\n\nn_hotels['hotel_country'] = n_hotels['hotel_country'].replace(country_fix)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels['domestic_traveler_flag'] = (\n    n_hotels['hotel_country'].str.strip().str.lower() ==\n    n_hotels['reviewer_nationality'].str.strip().str.lower()\n).astype(int)\n\nprint(n_hotels['domestic_traveler_flag'].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"continent_map = {\n    'europe': ['united kingdom', 'france', 'germany', 'spain', 'italy', 'netherlands', 'belgium', 'sweden', 'norway', 'switzerland', 'portugal', 'poland', 'austria', 'greece', 'ireland', 'finland', 'czech republic', 'denmark', 'hungary', 'slovakia', 'romania', 'bulgaria', 'croatia', 'slovenia', 'iceland'],\n    'asia': ['china', 'india', 'japan', 'malaysia', 'singapore', 'thailand', 'south korea', 'indonesia', 'philippines', 'vietnam', 'pakistan', 'bangladesh'],\n    'americas': ['united states of america', 'canada', 'mexico', 'brazil', 'argentina', 'chile', 'colombia', 'peru'],\n    'middle_east': ['united arab emirates', 'saudi arabia', 'qatar', 'kuwait', 'bahrain', 'oman', 'lebanon'],\n    'africa': ['south africa', 'morocco', 'egypt', 'kenya', 'nigeria', 'ghana'],\n    'oceania': ['australia', 'new zealand']\n}\n\ndef get_continent(country):\n    if pd.isna(country) or country.strip() == '':\n        return 'unknown'\n    country = country.strip().lower()\n    for cont, countries in continent_map.items():\n        if country in countries:\n            return cont\n    return 'other'\n\nn_hotels['hotel_continent'] = n_hotels['hotel_country'].apply(get_continent)\nn_hotels['reviewer_continent'] = n_hotels['reviewer_nationality'].apply(get_continent)\n\nn_hotels['cross_region_flag'] = (\n    (n_hotels['hotel_continent'] != n_hotels['reviewer_continent']).astype(int)\n)\n\nprint(n_hotels[['hotel_country', 'reviewer_nationality', 'domestic_traveler_flag']].sample(10))\nprint(n_hotels['cross_region_flag'].value_counts(normalize=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop address columns (typos and duplicates)\nn_hotels.drop(['hotel_name', 'hotel_address'], axis=1, inplace=True)\n\n# Final check\nprint(\"Remaining missing coordinates:\")\nprint(n_hotels[['lat', 'lng']].isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Nationality into Travel distance","metadata":{}},{"cell_type":"code","source":"# Dictionary mapping reviewer nationalities to approximate lat/lng coordinates\nreviewer_nationality_coords = {\n    ' United Kingdom ': (55.3781, -3.4360),\n    ' Belgium ': (50.5039, 4.4699),\n    ' Sweden ': (60.1282, 18.6435),\n    ' United States of America ': (37.0902, -95.7129),\n    ' Ecuador ': (-1.8312, -78.1834),\n    ' Netherlands ': (52.1326, 5.2913),\n    ' Ireland ': (53.1424, -7.6921),\n    ' Canada ': (56.1304, -106.3468),\n    ' Norway ': (60.4720, 8.4689),\n    ' Bulgaria ': (42.7339, 25.4858),\n    ' Italy ': (41.8719, 12.5674),\n    ' Australia ': (-25.2744, 133.7751),\n    ' Seychelles ': (-4.6796, 55.4920),\n    ' Kuwait ': (29.3759, 47.9774),\n    ' Saudi Arabia ': (23.8859, 45.0792),\n    ' Czech Republic ': (49.8175, 15.4730),\n    ' France ': (46.2276, 2.2137),\n    ' Germany ': (51.1657, 10.4515),\n    ' South Africa ': (-30.5595, 22.9375),\n    ' United Arab Emirates ': (23.4241, 53.8478),\n    ' Greece ': (39.0742, 21.8243),\n    ' Spain ': (40.4637, -3.7492),\n    ' Switzerland ': (46.8182, 8.2275),\n    ' Macedonia ': (41.6086, 21.7453),\n    ' Poland ': (51.9194, 19.1451),\n    ' Bahrain ': (26.0667, 50.5577),\n    ' Qatar ': (25.3548, 51.1839),\n    ' India ': (20.5937, 78.9629),\n    ' Singapore ': (1.3521, 103.8198),\n    ' Malaysia ': (4.2105, 101.9758),\n    ' Thailand ': (15.8700, 100.9925),\n    ' Brazil ': (-14.2350, -51.9253),\n    ' Crimea ': (45.0, 34.0),\n    ' Turkey ': (38.9637, 35.2433),\n    ' Israel ': (31.0461, 34.8516),\n    ' ': (None, None),\n    ' Lebanon ': (33.8547, 35.8623),\n    ' Romania ': (45.9432, 24.9668),\n    ' Cyprus ': (35.1264, 33.4299),\n    ' Portugal ': (39.3999, -8.2245),\n    ' Slovakia ': (48.6690, 19.6990),\n    ' Jersey ': (49.2144, -2.1313),\n    ' Gibraltar ': (36.1408, -5.3536),\n    ' Austria ': (47.5162, 14.5501),\n    ' Kenya ': (-0.0236, 37.9062),\n    ' Isle of Man ': (54.2361, -4.5481),\n    ' Costa Rica ': (9.7489, -83.7534),\n    ' Oman ': (21.5126, 55.9233),\n    ' Hungary ': (47.1625, 19.5033),\n    ' Iceland ': (64.9631, -19.0208),\n    ' Estonia ': (58.5953, 25.0136),\n    ' Hong Kong ': (22.3964, 114.1095),\n    ' China ': (35.8617, 104.1954),\n    ' Malta ': (35.9375, 14.3754),\n    ' Pakistan ': (30.3753, 69.3451),\n    ' Montenegro ': (42.7087, 19.3744),\n    ' Slovenia ': (46.1512, 14.9955),\n    ' South Korea ': (35.9078, 127.7669),\n    ' Ukraine ': (48.3794, 31.1656),\n    ' Japan ': (36.2048, 138.2529),\n    ' Azerbaijan ': (40.1431, 47.5769),\n    ' Russia ': (61.5240, 105.3188),\n    ' Brunei ': (4.5353, 114.7277),\n    ' Cayman Islands ': (19.3133, -81.2546),\n    ' Serbia ': (44.0165, 21.0059),\n    ' Argentina ': (-38.4161, -63.6167),\n    ' Denmark ': (56.2639, 9.5018),\n    ' Egypt ': (26.8206, 30.8025),\n    ' Finland ': (61.9241, 25.7482),\n    ' Mexico ': (23.6345, -102.5528),\n    ' Taiwan ': (23.6978, 120.9605),\n    ' Peru ': (-9.1899, -75.0152),\n    ' Philippines ': (12.8797, 121.7740),\n    ' New Zealand ': (-40.9006, 174.8860),\n    ' Luxembourg ': (49.8153, 6.1296),\n    ' Morocco ': (31.7917, -7.0926),\n    ' Latvia ': (56.8796, 24.6032),\n    ' Armenia ': (40.0691, 45.0382),\n    ' Indonesia ': (-0.7893, 113.9213),\n    ' Mauritius ': (-20.3484, 57.5522),\n    ' Croatia ': (45.1, 15.2),\n    ' Iraq ': (33.2232, 43.6793),\n    ' Namibia ': (-22.9576, 18.4904),\n    ' Iran ': (32.4279, 53.6880),\n    ' Bangladesh ': (23.6850, 90.3563),\n    ' Kosovo ': (42.6026, 20.9026),\n    ' Tunisia ': (33.8869, 9.5375),\n    ' Kazakhstan ': (48.0196, 66.9237),\n    ' Sri Lanka ': (7.8731, 80.7718),\n    ' Senegal ': (14.4974, -14.4524),\n    ' Guernsey ': (49.4657, -2.5857),\n    ' Bosnia and Herzegovina ': (43.9159, 17.6791),\n    ' Chile ': (-35.6751, -71.5430),\n    ' Jordan ': (30.5852, 36.2384),\n    ' Lithuania ': (55.1694, 23.8813),\n    ' Trinidad and Tobago ': (10.6918, -61.2225),\n    ' Albania ': (41.1533, 20.1683),\n    ' Yemen ': (15.5527, 48.5164),\n    ' Vietnam ': (14.0583, 108.2772),\n    ' Macau ': (22.1987, 113.5439),\n    ' Abkhazia Georgia ': (43.0, 41.0),\n    ' Puerto Rico ': (18.2208, -66.5901),\n    ' Nigeria ': (9.0820, 8.6753),\n    ' Georgia ': (42.3154, 43.3569),\n    ' Guatemala ': (15.7835, -90.2308),\n    ' Syria ': (34.8021, 38.9968),\n    ' Cura ao ': (12.1696, -68.9900),\n    ' El Salvador ': (13.7942, -88.8965),\n    ' Monaco ': (43.7336, 7.4170),\n    ' Algeria ': (28.0339, 1.6596),\n    ' Belarus ': (53.7098, 27.9534),\n    ' Maldives ': (3.2028, 73.2207),\n    ' Colombia ': (4.5709, -74.2973),\n    ' Mauritania ': (21.0079, -10.9408),\n    ' Venezuela ': (6.4238, -66.5897),\n    ' Kyrgyzstan ': (41.2044, 74.7661),\n    ' Libya ': (26.3351, 17.2283),\n    ' Saint Lucia ': (13.9094, -60.9789),\n    ' Tanzania ': (-6.3690, 34.8888),\n    ' Andorra ': (42.5462, 1.6016),\n    ' Fiji ': (-17.7134, 178.0650),\n    ' Moldova ': (47.4116, 28.3699),\n    ' Panama ': (8.5380, -80.7821),\n    ' Grenada ': (12.1165, -61.6790),\n    ' Angola ': (-11.2027, 17.8739),\n    ' Ghana ': (7.9465, -1.0232),\n    ' Sudan ': (12.8628, 30.2176),\n    ' Ivory Coast ': (7.539989, -5.5471),\n    ' Myanmar ': (21.9162, 95.9560),\n    ' Aruba ': (12.5211, -69.9683),\n    ' Uruguay ': (-32.5228, -55.7658),\n    ' U S Virgin Islands ': (18.3358, -64.8963),\n    ' Mongolia ': (46.8625, 103.8467),\n    ' Haiti ': (18.9712, -72.2852),\n    ' Tajikistan ': (38.8610, 71.2761),\n    ' Cambodia ': (12.5657, 104.9910),\n    ' Uzbekistan ': (41.3775, 64.5853),\n    ' Dominican Republic ': (18.7357, -70.1627),\n    ' Bermuda ': (32.3078, -64.7505),\n    ' United States Minor Outlying Islands ': (28.5, -177.0),\n    ' Uganda ': (1.3733, 32.2903),\n    ' Bahamas ': (25.0343, -77.3963),\n    ' Guyana ': (4.8604, -58.9302),\n    ' Barbados ': (13.1939, -59.5432),\n    ' Zimbabwe ': (-19.0154, 29.1549),\n    ' Saint Martin ': (18.0708, -63.0501),\n    ' Palestinian Territory ': (31.9522, 35.2332),\n    # ... continue for the remaining territories as needed\n}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import json\n\n# Save to JSON file\n#with open(\"reviewer_nationality_coords.json\", \"w\") as f:\n#    json.dump(reviewer_nationality_coords, f)\n\n# Load dictionary from JSON file\n#with open(\"/kaggle/working/reviewer_nationality_coords.json\", \"r\") as f:\n#    reviewer_nationality_coords = json.load(f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent=\"HotelReviewer\", timeout=2)  # increase to 10 seconds\n\n\nimport time\n\ndef get_coordinates(country):\n    try:\n        #country = country.strip()\n        if country in reviewer_nationality_coords:\n            return tuple(reviewer_nationality_coords[country])  # convert list back to tuple\n        if country == '':\n            return ((None, None))\n        #location = geolocator.geocode(f\"{country}, {country}\", exactly_one=True, timeout=10)\n        #if location:\n        #    return location.latitude, location.longitude\n        else:\n            return ((None, None))\n    except Exception as e:\n        print(f\"Error for country {country}: {e}\")\n        return ((None, None))\n\nreviewer_nationality_list = n_hotels['reviewer_nationality'].unique()\nreviewer_nationality_dict = {country: get_coordinates(country) for country in reviewer_nationality_list}\n\nn_hotels['lat_nationality'], n_hotels['lng_nationality'] = zip(*n_hotels['reviewer_nationality'].map(reviewer_nationality_dict))\n\nmedian_fill = {\n            'lat_nationality': n_hotels['lat_nationality'].median(),\n            'lng_nationality': n_hotels['lng_nationality'].median()\n            }\n\nn_hotels = n_hotels.fillna(median_fill)\n'''\n# Replace (0,0) with NaN\nn_hotels['lat_nationality'] = n_hotels['lat_nationality'].replace(0, np.nan)\nn_hotels['lng_nationality'] = n_hotels['lng_nationality'].replace(0, np.nan)\n\n# Create a flag for missing coordinates\nn_hotels['nationality_missing'] = n_hotels['lat_nationality'].isna().astype(int)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select rows where either latitude or longitude is NaN\nmissing_coords = n_hotels[n_hotels['lat_nationality'].isna() | n_hotels['lng_nationality'].isna()]\n\n# Count by nationality\nmissing_by_country = missing_coords['reviewer_nationality'].value_counts()\nprint(missing_by_country)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Local visitors often have different expectations → tend to rate differently.\n\nFar travelers might visit more expensive or tourist-oriented hotels → correlation with higher/lower reviewer_score.\n\nIt also serves as a proxy for tourism type or international popularity.","metadata":{}},{"cell_type":"code","source":"from geopy.distance import geodesic\nimport numpy as np\nimport pandas as pd\n\ndef compute_distance(row):\n    \"\"\"\n    Compute travel distance between nationality (home) and hotel coordinates in kilometers.\n    Returns NaN if any coordinate is missing.\n    \"\"\"\n    try:\n        if (pd.notnull(row['lat_nationality']) and pd.notnull(row['lng_nationality']) and\n            pd.notnull(row['lat']) and pd.notnull(row['lng'])):\n            \n            # Skip placeholder (0,0)\n            if (row['lat_nationality'] == 0 and row['lng_nationality'] == 0):\n                return np.nan\n            \n            home = (row['lat_nationality'], row['lng_nationality'])\n            hotel = (row['lat'], row['lng'])\n            return geodesic(home, hotel).km  # returns distance in kilometers\n        else:\n            return np.nan\n    except Exception as e:\n        print(f\"Error computing distance for row: {e}\")\n        return np.nan\n\n# Apply the function to each row\nn_hotels['travel_distance_km'] = n_hotels.apply(compute_distance, axis=1)\n\n# Optional: fill missing distances with median\nmedian_distance = n_hotels['travel_distance_km'].median()\nn_hotels['travel_distance_km'] = n_hotels['travel_distance_km'].fillna(median_distance)\n\nprint(\" Travel distance feature added!\")\nprint(n_hotels[['reviewer_nationality', 'travel_distance_km']].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter for reviewer_nationality = 'Russia'\n#hotels_russia = n_hotels[n_hotels['reviewer_nationality'] == 'Russia']\n\n# Display address (if exists), latitude, and longitude\n#display(hotels_russia[['reviewer_nationality', 'lat_nationality', 'lng_nationality']].head())\n\nn_hotels.drop(columns=['reviewer_nationality'], inplace=True)\nn_hotels.drop(columns=['lat_nationality','lng_nationality'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Step: Create Travel Zone feature\n# -------------------------------\n\ndef assign_travel_zone(distance_km):\n    \"\"\"Assign travel zone based on distance (in km).\"\"\"\n    if pd.isna(distance_km):\n        return 0   # Unknown / missing\n    elif distance_km < 150:\n        return 1   # Local trip\n    elif distance_km < 1000:\n        return 2   # Nearby / regional trip\n    else:\n        return 3   # Long-distance / international trip\n\n# Apply the function\nn_hotels['travel_zone'] = n_hotels['travel_distance_km'].apply(assign_travel_zone)\n\n# Check distribution\nprint(n_hotels['travel_zone'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zone_counts = n_hotels['travel_zone'].value_counts().sort_index()\nzone_percent = (zone_counts / len(n_hotels) * 100).round(2)\n\nzone_summary = pd.DataFrame({\n    'Zone': zone_counts.index,\n    'Count': zone_counts.values,\n    'Percent': zone_percent.values\n})\n\nprint(zone_summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nn_hotels = n_hotels.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tag Processing","metadata":{}},{"cell_type":"markdown","source":"Approach:\n\n- Avoids repeated column insertions → no PerformanceWarning.\n\n- Handles missing values gracefully.\n\n- Easy to maintain and scalable for large numbers of tags.\n\n- Compatible directly with ML models.","metadata":{}},{"cell_type":"code","source":"n_hotels = nn_hotels.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels['tags'].head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# TAGS CLEANING AND FEATURE EXTRACTION\n# ============================================\n\nfrom collections import Counter\nimport category_encoders as ce\n\n\n# -------------------------------\n# Step 1: Extract number of nights stayed\n# -------------------------------\nimport re\n\n# Extract the number of nights from tags, e.g., \"Stayed 3 nights\"\nregex_nights = r'Stayed (\\d+) night[s]?'\nn_hotels['nights'] = n_hotels['tags'].str.extract(regex_nights)\nn_hotels['nights'] = n_hotels['nights'].fillna(0).astype(int)\n\n# Remove nights-related tags from the 'tags' column\nn_hotels['tags'] = n_hotels['tags'].str.replace(regex_nights, '', regex=True)\n\n# -------------------------------\n# Step 2: Pet ownership\n# -------------------------------\nn_hotels['pet'] = n_hotels['tags'].str.contains('With a pet', regex=False).astype(int)\nn_hotels['tags'] = n_hotels['tags'].str.replace('With a pet', '', regex=False)\n\n# -------------------------------\n# Step 3: Booking from mobile device\n# -------------------------------\nn_hotels['from_mobile_device'] = n_hotels['tags'].str.contains('Submitted from a mobile device', regex=False).astype(int)\nn_hotels['tags'] = n_hotels['tags'].str.replace('Submitted from a mobile device', '', regex=False)\n\n# -------------------------------\n# Step 4: Trip type\n# -------------------------------\n# Extract trip type tags, e.g., \"Leisure trip\", \"Business trip\"\nregex_trip = r\"(Leisure trip|Business trip|.*? trip)\"\nn_hotels['trip'] = n_hotels['tags'].str.extract(regex_trip)\n# Encode trip type as ordinal\ntrip_encoder = ce.OrdinalEncoder()\nn_hotels['trip'] = trip_encoder.fit_transform(n_hotels[['trip']])\n# Remove trip tags from 'tags' column\nn_hotels['tags'] = n_hotels['tags'].str.replace(regex_trip, '', regex=True)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Step 5: Simplified Traveler type\n# -------------------------------\n\ndef categorize_traveler(tag_str):\n    \"\"\"\n    Extract traveler type from tag string and simplify to:\n    1 = solo, 2 = couple, 3 = group/with others, 0 = unknown\n    \"\"\"\n    if pd.isna(tag_str) or tag_str.strip() == '':\n        return 0\n\n    # Extract the first meaningful tag\n    try:\n        tag = tag_str.split(',')[0].split(\"'\")[1].strip()\n    except IndexError:\n        return 0\n\n    tag_lower = tag.lower()\n    if 'solo' in tag_lower:\n        return 1\n    elif 'couple' in tag_lower:\n        return 2\n    elif 'group' in tag_lower or 'with' in tag_lower:\n        return 3\n    else:\n        return 0\n\n# Apply the function\nn_hotels['traveler_type'] = n_hotels['tags'].apply(categorize_traveler)\n\n# Show distribution\ntraveler_type_counts = n_hotels['traveler_type'].value_counts().sort_index().reset_index()\ntraveler_type_counts.columns = ['Traveler_Type', 'Count']\nprint(\"Simplified traveler types:\")\nprint(traveler_type_counts)\n\n# Remove traveler_type tag from 'tags' column (optional cleanup)\ntraveler_type_original = ['Couple','Solo traveler','Family with young children','Group','Family with older children','Travelers with friends']\nfor key in traveler_type_original: #traveler_type_counts['Traveler_Type'].unique():\n    n_hotels['tags'] = n_hotels['tags'].str.replace(str(key), '', regex=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove traveler_type tag from 'tags' column\n#for key in n_hotels['traveler_type'].unique():\n#    n_hotels['tags'] = n_hotels['tags'].str.replace(str(key), '', regex=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Step 6: Non-smoking feature\n# -------------------------------\nn_hotels['NonSmoking'] = n_hotels['tags'].str.contains('Non Smoking', regex=False).astype(int)\nn_hotels['tags'] = n_hotels['tags'].str.replace('Non Smoking', '', regex=False)\n\n# -------------------------------\n# Step 7: Clean leftover tags string\n# -------------------------------\nn_hotels['tags'] = n_hotels['tags'].str.replace('[\\[\\]\\'\\,]', '', regex=True)\nn_hotels['tags'] = n_hotels['tags'].str.strip()\n\n# -------------------------------\n# Final backup\n# -------------------------------\nhotels01 = n_hotels.copy()\n\n# Check processed features\nprint(n_hotels[['nights', 'pet', 'from_mobile_device', 'trip', 'traveler_type', 'NonSmoking']].head(5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. Clean and Split Tags Properly\n\nEach tag string might contain multiple supplies separated by spaces, commas, or multiple spaces.\nLet’s normalize that first.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter\nimport re\n\n# Example: ensure the column is treated as string and handle NaNs\nn_hotels['tags'] = n_hotels['tags'].astype(str).fillna('')\n\n# Split tags — this version is robust against multiple spaces or commas\ndef split_tags(tag_string):\n    # Remove quotes and \"None\"\n    tag_string = tag_string.strip().replace(\"None\", \"\").strip(\"'\\\"\")\n    if not tag_string:\n        return []\n    # Split by 2+ spaces, commas, or semicolons\n    items = re.split(r'\\s{2,}|,|;', tag_string)\n    # Strip whitespace around each tag and remove empties\n    return [item.strip() for item in items if item.strip()]\n\n# Apply to all rows\nn_hotels['tag_list'] = n_hotels['tags'].apply(split_tags)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Count Frequency of Each Tag\n\nfrom collections import Counter\n\n# Flatten list of lists into a single list of tags\nall_tags = [tag for tags in n_hotels['tag_list'] for tag in tags]\n\n# Count frequency\nsupplies_counter = Counter(all_tags)\n\n# Show number of unique tags and top 20\nprint(f\"Number of unique supplies: {len(supplies_counter)}\")\nprint(supplies_counter.most_common(20))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Get popular supplies\n\npopular_supplies = [k for k, v in supplies_counter.items() if v > 500]\nprint(f\"Popular supplies (occurrences > 500): {len(popular_supplies)}\")\nprint(popular_supplies)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Convert to DataFrame for plotting\nsupplies_df = pd.DataFrame(supplies_counter.most_common(40), columns=['Supply', 'Count'])\n\nplt.figure(figsize=(10,12))\nplt.barh(supplies_df['Supply'], supplies_df['Count'])\nplt.gca().invert_yaxis()\nplt.title(\"Top 20 Most Common Hotel Tags\")\nplt.xlabel(\"Count\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# ROOM FEATURES EXTRACTION\n# ============================================\n\nfrom collections import Counter\n\n\n# -------------------------------\n# Step 1: Split 'tags' into 'room' and 'supplies'\n# -------------------------------\ndef extract_room_supplies(tag_str):\n    \"\"\"Split tag string by 'with': room type vs extra supplies/view\"\"\"\n    parts = tag_str.split('with')\n    if len(parts) == 1:\n        return parts[0].strip(), None\n    else:\n        return parts[0].strip(), parts[1].replace('view', 'View').strip()\n\nn_hotels['room'], n_hotels['supplies'] = zip(*n_hotels['tags'].apply(extract_room_supplies))\n\n# -------------------------------\n# Step 2: Room type (Single, Double/Twin, Triple, Studio, Suite)\n# -------------------------------\ndef categorize_room_type(room_str):\n    if 'Single' in room_str: return 1\n    elif any(x in room_str for x in ['Double or Twin', 'Double', 'Twin', '2 rooms']): return 2\n    elif any(x in room_str for x in ['3 rooms', 'Triple Room']): return 3\n    elif 'Studio' in room_str: return 1\n    elif 'Suite' in room_str: return 1\n    else: return 0\n\nn_hotels['room_type'] = n_hotels['room'].apply(categorize_room_type)\nprint(Counter(n_hotels['room_type']))\n\n# -------------------------------\n# Step 3: Room description (Standard, Family, Queen, King, Luxury, Deluxe, Executive, Superior)\n# -------------------------------\ndef categorize_room_description(room_str):\n    if any(x in room_str for x in ['Standard', 'Ordinary', 'Classic']): return 1\n    elif 'Family' in room_str: return 2\n    elif 'Queen' in room_str: return 3\n    elif 'King' in room_str: return 3\n    elif any(x in room_str for x in ['Luxury', 'Deluxe', 'Executive', 'Superior']): return 3\n    else: return 0\n\nn_hotels['room_description'] = n_hotels['room'].apply(categorize_room_description)\nprint(Counter(n_hotels['room_description']))\n\n# -------------------------------\n# Step 4: Room view (binary)\n# -------------------------------\nn_hotels['view'] = n_hotels['supplies'].str.contains('View', regex=False).fillna(0).astype(int)\nprint(Counter(n_hotels['view']))\n\n# -------------------------------\n# Step 5: Analyze extra supplies (optional)\n# -------------------------------\nsupplies_counter = Counter(n_hotels['supplies'])\npopular_supplies = [k for k, v in supplies_counter.items() if v > 500]\nprint(f\"Number of unique supplies: {len(supplies_counter)}\")\nprint(f\"Popular supplies (occurrences >500): {len(popular_supplies)}\")\nprint(popular_supplies)\n\n# -------------------------------\n# Step 6: Drop unneeded columns\n# -------------------------------\n#n_hotels.drop(['tags', 'supplies', 'room'], axis=1, inplace=True)\nn_hotels.info()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\n\n# ==========================================\n#  Long stay flag\n# ==========================================\nn_hotels['long_stay_flag'] = (n_hotels['nights'] > 5).astype(int)\n\n\n\n# ==========================================\n#  Reviewer experience (log transform)\n# ==========================================\nn_hotels['reviewer_experience'] = np.log1p(\n    n_hotels['total_number_of_reviews_reviewer_has_given']\n)\n\n# ==========================================\n#  Family flag (from tags)\n# ==========================================\nn_hotels['family_flag'] = n_hotels['tags'].str.contains(\n    'family|children', case=False, na=False\n).astype(int)\n\n# ==========================================\n#  Room quality flag (from tags)\n# ==========================================\nn_hotels['room_quality_flag'] = n_hotels['tags'].str.contains(\n    'deluxe|suite|superior|executive', case=False, na=False\n).astype(int)\n\n\n\n# ==========================================\n#  Quick sanity check\n# ==========================================\nfeature_summary = n_hotels[[\n    'domestic_traveler_flag',\n    'cross_region_flag',\n    'long_stay_flag',\n    'reviewer_experience',\n    'family_flag',\n    'room_quality_flag'\n]].head()\n\nprint(feature_summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoding of Reviews","metadata":{}},{"cell_type":"code","source":"# ============================================\n# SENTIMENT ENCODING OF REVIEWS\n# ============================================\n\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Download VADER lexicon if not already present\nnltk.download('vader_lexicon')\n\n# Initialize the VADER sentiment analyzer\nsent_analyzer = SentimentIntensityAnalyzer()\n\n# Function to compute the compound sentiment score\ndef get_compound_score(text):\n    \"\"\"Return the VADER compound sentiment score for a given text.\"\"\"\n    return sent_analyzer.polarity_scores(text)['compound']\n\n# Apply sentiment analysis to positive and negative reviews\nn_hotels['positive_score'] = n_hotels['positive_review'].apply(get_compound_score)\nn_hotels['negative_score'] = n_hotels['negative_review'].apply(get_compound_score)\n\n# Drop the original text columns as they are now encoded\nn_hotels.drop(['positive_review', 'negative_review'], axis=1, inplace=True)\n\n#=========\n# Create a new feature for the difference between positive and negative word counts\nn_hotels['pos_vs_neg_words'] = (\n    n_hotels['review_total_positive_word_counts'] - n_hotels['review_total_negative_word_counts']\n)\n\n# Optional: explore the distribution\nprint(n_hotels['pos_vs_neg_words'].describe())\n\n# Optional: visualize\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,4))\nn_hotels['pos_vs_neg_words'].hist(bins=50)\nplt.title('Distribution of Positive vs Negative Word Difference')\nplt.xlabel('Positive - Negative Word Count')\nplt.ylabel('Frequency')\nplt.show()\n\n#========\n# Create a new feature for the ratio between positive and negative word counts\nn_hotels['pos_to_neg_ratio'] = (\n    (n_hotels['review_total_positive_word_counts'] + 1) /\n    (n_hotels['review_total_negative_word_counts'] + 1)\n)\n\n# Check updated dataframe\nn_hotels.info()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# ============================================\n# TAGS PREPROCESSING\n# ============================================\n\n# Convert the string representation of tags to a list\ndef parse_tags(tag_str):\n    \"\"\"Convert tag string to list of individual tags\"\"\"\n    if pd.isnull(tag_str):\n        return []\n    tag_str = tag_str[2:-2]  # remove the leading/trailing brackets\n    return tag_str.strip().split(\"', '\")\n\nn_hotels['tags_n'] = n_hotels['tags'].apply(parse_tags)\n\n# Explode the lists to analyze tag frequencies\nexplode = n_hotels.explode('tags_n')\n\n# Count frequency of each tag\ntag_counts = explode['tags_n'].value_counts()\n\n# Select only popular tags (appearing more than 900 times)\npopular_tags = tag_counts[tag_counts > 900].index.tolist()\n\nprint(f\"Total unique tags: {explode['tags_n'].nunique()}\")\nprint(f\"Selected top frequent tags: {len(popular_tags)}\")\n\n# --------------------------------------------\n# Efficient binary encoding of top tags\n# --------------------------------------------\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer(classes=popular_tags)\ntags_encoded = pd.DataFrame(\n    mlb.fit_transform(n_hotels['tags_n']),\n    columns=mlb.classes_,\n    index=n_hotels.index\n)\n\n# Merge the encoded tags into the main dataframe\nn_hotels = pd.concat([n_hotels, tags_encoded], axis=1)\n\n# Optional: drop the original columns if not needed\nn_hotels = n_hotels.drop(['tags', 'tags_n'], axis=1)\n\n# Check the new dataframe\nn_hotels.head(3)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =====================================================\n# 9. Transform Days Since Review\n# =====================================================\ndef extract_days(x):\n    try:\n        return int(str(x)[:3])\n    except ValueError:\n        return int(str(x)[:2])\n\nn_hotels['days_since_review_n'] = n_hotels['days_since_review'].apply(extract_days)\n\n# ==========================================\n#  Recency weight\n# ==========================================\nn_hotels['recency_weight'] = 1 / np.log1p( n_hotels['days_since_review_n'] + 1 )\n\nn_hotels.drop(columns=['days_since_review'], inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# =====================================================\n#  Step: Correlation Between Tags and Ratings\n# =====================================================\n# Goal:\n# To find which review tags (like \"Business trip\", \"Solo traveler\", etc.)\n# are most positively or negatively associated with hotel ratings.\n\n# Select only tag-related columns (they are binary: 0/1)\ntag_cols = popular_tags  # list of top tags created earlier\ntag_data = n_hotels[tag_cols + ['reviewer_score', 'average_score']]\n\n# Compute correlation matrix\ncorr_matrix = tag_data.corr()\n\n# Extract correlations of tags with reviewer_score and average_score\ncorr_with_reviewer = corr_matrix['reviewer_score'].sort_values(ascending=False)\ncorr_with_average = corr_matrix['average_score'].sort_values(ascending=False)\n\n# Combine results into one DataFrame for easy comparison\ntag_corr = pd.DataFrame({\n    'Corr_with_ReviewerScore': corr_with_reviewer[tag_cols],\n    'Corr_with_AverageScore': corr_with_average[tag_cols]\n}).sort_values(by='Corr_with_ReviewerScore', ascending=False)\n\ndisplay(tag_corr.head(10))\n\n# =====================================================\n#  Visualization: Tag Correlation with Ratings\n# =====================================================\n\nplt.figure(figsize=(10,20))\nsns.barplot(\n    x='Corr_with_ReviewerScore', \n    y=tag_corr.index, \n    data=tag_corr, \n    palette='viridis'\n)\nplt.title('Correlation Between Tags and Reviewer Scores')\nplt.xlabel('Correlation Coefficient')\nplt.ylabel('Tag')\nplt.tight_layout()\nplt.show()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Date-Based Features","metadata":{}},{"cell_type":"code","source":"#n_hotels['year'] = n_hotels['review_date'].dt.year\n\ndef get_season(date):\n    if date.month in [12, 1, 2]:\n        return 'winter'\n    elif date.month in [3, 4, 5]:\n        return 'spring'\n    elif date.month in [6, 7, 8]:\n        return 'summer'\n    else:\n        return 'autumn'\n\nn_hotels['season'] = n_hotels['review_date'].apply(get_season)\n# ==========================================\n#  Season encoded (cyclical transformation)\n# sin/cos encoding helps models understand seasonality\n# ==========================================\nseason_map = {'spring': 0, 'summer': 1, 'autumn': 2, 'winter': 3}\nn_hotels['season_num'] = n_hotels['season'].str.lower().map(season_map)\n\nn_hotels['season_sin'] = np.sin(2 * np.pi * n_hotels['season_num'] / 4)\nn_hotels['season_cos'] = np.cos(2 * np.pi * n_hotels['season_num'] / 4)\n\n\nn_hotels.drop(columns=['review_date'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-hot encode the 'season' column\nseason_dummies = pd.get_dummies(n_hotels['season'], prefix='season')\n\n# Join encoded columns back to the dataset\nn_hotels = pd.concat([n_hotels, season_dummies], axis=1)\n\n# Drop the original 'season' column\n#n_hotels.drop('season', axis=1, inplace=True)\n\n# Check result\nn_hotels.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keep only numeric columns\n#n_hotels_numeric = n_hotels.select_dtypes(include=['number']).copy()\n\n# Check resulting columns\n#print(n_hotels_numeric.columns)\n\nn_hotels = n_hotels.select_dtypes(include=['number'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory data analysis (EDA)\nBefore feeding our cleaned dataset into a machine learning model, it’s crucial to analyze it and understand the distributions, potential outliers, and correlations. This step helps avoid garbage-in, garbage-out scenarios.","metadata":{}},{"cell_type":"code","source":"# Check missing values\nmissing = n_hotels.isnull().sum()\nmissing = missing[missing > 0]\nprint(\"Columns with missing values:\\n\", missing)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Basic statistics","metadata":{}},{"cell_type":"code","source":"# Summary statistics for numerical columns\nprint(n_hotels.describe().T)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Log-transform skewed numerical features","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nskewed_cols = ['days_since_review_n', 'total_number_of_reviews', 'additional_number_of_scoring']\n\nfor col in skewed_cols:\n    n_hotels[col + '_log'] = np.log1p(n_hotels[col])  # log(1+x) to handle zeros\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_hotels[['days_since_review_n', 'days_since_review_n_log']].hist(figsize=(12,4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize distributions of numerical features","metadata":{}},{"cell_type":"code","source":"n_hotels.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#n_hotels.drop('tag_list', axis=1, inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Identify categorical and numeric columns\n# ==========================\n\n# categorical features\ncat_cols = [ 'sample', 'pet', 'from_mobile_device', 'trip', 'traveler_type', \n            'NonSmoking', 'room_type', 'room_description', 'view', 'positive_score',\n            'negative_score', 'season_autumn', 'season_spring', 'season_summer', 'season_winter']\n\n# numerical features\nnum_cols = list(n_hotels.drop(columns=cat_cols, axis=1))\n\nnum_cols, cat_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#num_cols = n_hotels.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nplt.figure(figsize=(16, 20))\nfor i, col in enumerate(num_cols, 1):\n    plt.subplot(len(num_cols)//3 + 1, 3, i)\n    sns.boxplot(x=n_hotels[col])\n    plt.title(col)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Outliers\n\n**Tree-based models** split by thresholds, so extreme values won’t “pull” the model the way they would in regression.","metadata":{}},{"cell_type":"code","source":"#   3-sigma method (Tukey's method) to identify outliers in numerical features\nimport pandas as pd\n\ndef outliers_iqr(data, feature, k=3.0):\n    \"\"\"Detect and remove outliers using the Tukey IQR method.\"\"\"\n    x = data[feature]\n    q1, q3 = x.quantile(0.25), x.quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - k * iqr\n    upper_bound = q3 + k * iqr\n    outliers = data[(x < lower_bound) | (x > upper_bound)]\n    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]\n    return outliers, cleaned\n\n# Create a cleaned copy of the dataset\ndata_cleaner = n_hotels.copy()\n\n# Loop through numeric columns\nfor feature in num_cols:\n    outliers, cleaned = outliers_iqr(data_cleaner, feature, k=3)\n    print(f\"{feature}: removed {len(outliers)} outliers\")\n    data_cleaner = cleaned  # progressively clean the dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 20))\nfor i, col in enumerate(num_cols, 1):\n    plt.subplot(len(num_cols)//3 + 1, 3, i)\n    sns.boxplot(x=data_cleaner[col])\n    plt.title(col)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(len(num_cols), 2, figsize=(12, 4 * len(num_cols)))\n\nfor i, col in enumerate(num_cols):\n    # Before cleaning\n    axes[i, 0].hist(n_hotels[col], bins=50, color='skyblue', alpha=0.7)\n    axes[i, 0].set_title(f'{col} — Before Cleaning')\n\n    # After cleaning\n    axes[i, 1].hist(data_cleaner[col], bins=50, color='lightgreen', alpha=0.7)\n    axes[i, 1].set_title(f'{col} — After Cleaning')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a cleaned copy of the dataset\nn_hotels_cleaner = n_hotels.copy()\n\n# It seems like, some of the features do require outlier cleaning\n# --- Apply cleaning only to selected columns ---\nout_cols = [\n    'total_number_of_reviews',\n    'review_total_negative_word_counts',\n    'review_total_positive_word_counts',\n    'total_number_of_reviews_reviewer_has_given',\n    'nights'\n]\n\nfor feature in out_cols:\n    outliers, cleaned = outliers_iqr(n_hotels, feature, k=3)\n    print(f\"{feature}: removed {len(outliers)} outliers\")\n    n_hotels_cleaner = cleaned  # progressively remove outliers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scaling\n\nFor numeric features that vary a lot in magnitude and will otherwise dominate distance-based or linear models.\n\n**Tree-based models** (RandomForest, XGBoost, etc.) are scale-invariant, so scaling is not needed.\nHowever, if you’ll use regression, PCA, or neural networks, scaling is highly recommended.\n","metadata":{}},{"cell_type":"code","source":"'''\nto_scale = [\n    'additional_number_of_scoring',\n    'review_total_negative_word_counts',\n    'review_total_positive_word_counts',\n    'total_number_of_reviews',\n    'total_number_of_reviews_reviewer_has_given',\n    'nights',\n    'days_since_review_n'\n]\nno_scale = [\n    'average_score',\n    'reviewer_score',\n    'lat', 'lng',\n    'lat_nationality', 'lng_nationality',\n    'year'\n]\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nn_hotels[to_scale] = scaler.fit_transform(n_hotels[to_scale])\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"'''\n\nimport numpy as np\n\ndef cap_outliers(df, columns, factor=1.5):\n    df = df.copy()\n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower = Q1 - factor * IQR\n        upper = Q3 + factor * IQR\n        # cap values\n        df[col] = np.where(df[col] < lower, lower,\n                           np.where(df[col] > upper, upper, df[col]))\n    return df\n\nn_hotels[num_cols] = cap_outliers(n_hotels, num_cols)\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create interaction features","metadata":{}},{"cell_type":"code","source":"# Weighted average influence\n\nn_hotels['avg_score_times_total_reviews'] = n_hotels['average_score'] * n_hotels['total_number_of_reviews']\nn_hotels['avg_score_times_additional_scoring'] = n_hotels['average_score'] * n_hotels['additional_number_of_scoring']\n\n# ratio\nn_hotels['positive_to_negative_ratio'] = (n_hotels['review_total_positive_word_counts'] + 1) / \\\n                                        (n_hotels['review_total_negative_word_counts'] + 1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation map","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute correlation only for numerical columns\ncorr = n_hotels.select_dtypes(include=['number']).corr()\n\n# Create a better visual\nplt.figure(figsize=(16,10))\nsns.heatmap(\n    corr, \n    cmap='coolwarm',       # better contrast\n    center=0,              # zero-centered colors\n    annot=False,           # can set True for small matrices\n    linewidths=0.5,\n    cbar_kws={'shrink': .7}\n)\nplt.title(\"Correlation Heatmap of Numerical Features\", fontsize=16)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute correlations\ncorr = n_hotels.corr(numeric_only=True)\n\n# Drop 'sample' and the target itself\nfeature_x='reviewer_score'\ntarget_corr = corr[feature_x].drop([feature_x, 'sample'], errors='ignore')\n\n# Sort correlations by absolute value (optional — helps visualize strongest relationships)\ntarget_corr = target_corr.sort_values(ascending=False)\n\n# Display values\nprint(target_corr)\n\n# Plot\nplt.figure(figsize=(8, 10))\nsns.barplot(y=target_corr.index, x=target_corr.values, palette='viridis')\nplt.title(\"Correlation with Reviewer Score (excluding 'sample' and target)\")\nplt.xlabel(\"Correlation\")\nplt.ylabel(\"Feature\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n#Scatter plots for relationships\n\nplt.figure(figsize=(12,6))\nsns.scatterplot(x='nights', y='reviewer_score', data=n_hotels, alpha=0.3)\nplt.title('Reviewer Score vs Nights Stayed')\nplt.show()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train/Test Split and Model Training","metadata":{}},{"cell_type":"code","source":"data_chosen = n_hotels.copy()\n#data_chosen = n_hotels_cleaner.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_chosen.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n# 10. Train/Test Split, Encoding, and Model Training\n# =====================================================\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nimport pandas as pd\n\n# Separate train and test data\ntrain_data = data_chosen.query('sample == 1').drop(['sample'], axis=1)\ntest_data = data_chosen.query('sample == 0').drop(['sample'], axis=1)\n\n# Target and features\ny = train_data['reviewer_score']\nX = train_data.drop(['reviewer_score'], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# ==========================\n# Encode categorical columns using One-Hot Encoding\n# ==========================\nX_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n\n# Apply the same encoding to the test set\nX_test_raw = test_data.drop(['reviewer_score'], axis=1)\nX_test_encoded = pd.get_dummies(X_test_raw, columns=categorical_cols, drop_first=True)\n\n# Ensure train and test have the same columns (fill missing columns with 0)\nX_test_encoded = X_test_encoded.reindex(columns=X_encoded.columns, fill_value=0)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Split train data into train/test sets\n# ==========================\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# ==========================\n# Optional: Scale numeric features (not required for Random Forest)\n# ==========================\n# scaler = MinMaxScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_valid_scaled = scaler.transform(X_valid)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Train Random Forest Model\n# ==========================\n#rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n#rf.fit(X_train, y_train)\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_grid = {\n    'n_estimators': [100], #, 200, 500\n    'max_depth': [30], #None, 10, 20, 30\n    'min_samples_split': [2],#, 5, 10\n    #'min_samples_leaf': [ 4], #1, 2,\n    #'max_features': ['auto']#, 'sqrt'\n}\n\nrf = RandomForestRegressor(random_state=42)\nsearch = RandomizedSearchCV(rf, param_distributions=param_grid,\n                            n_iter=20, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nbest_rf = search.best_estimator_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_rf = search.best_estimator_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Evaluate model\n# ==========================\ny_pred = best_rf.predict(X_valid)\nmape = metrics.mean_absolute_percentage_error(y_valid, y_pred) * 100\nmae = metrics.mean_absolute_error(y_valid, y_pred)\nrmse = metrics.mean_squared_error(y_valid, y_pred, squared=False)\nr2 = metrics.r2_score(y_valid, y_pred)\n\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R²: {r2:.3f}\")\nprint(f\"Test MAPE: {mape:.2f}%\")\n#MAE: 0.86\n#RMSE: 1.15\n#R²: 0.500\n#Test MAPE: 12.39%","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(best_rf.get_params()['n_estimators'])\nprint(best_rf.get_params()['max_depth'])\nprint(best_rf.get_params()['min_samples_split'])\nprint(best_rf.get_params()['min_samples_leaf'])\nprint(best_rf.get_params()['max_features'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visual inspection","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,6))\nplt.scatter(y_valid, y_pred, alpha=0.3)\nplt.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], 'r--')  # ideal line\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Predicted vs Actual\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## (Optional) Feature Importance Plot","metadata":{}},{"cell_type":"code","source":"# =====================================================\n# 12. Feature Importance Visualization\n# =====================================================\nimportances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(10,6))\nsns.barplot(x=importances.head(15), y=importances.head(15).index)\nplt.title('Top 15 Important Features - Random Forest')\nplt.xlabel('Importance Score')\nplt.ylabel('Feature')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}